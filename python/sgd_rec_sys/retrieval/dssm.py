"""
DSSM Deep Structured Semantic Models åŒå¡”æ¨¡å‹

åŸç†ï¼š
    * ç‰©å“å¡”ã€ç”¨æˆ·å¡”åˆ†åˆ«åšè¡¨å¾(ç‰¹å¾é¢„å¤„ç†ã€è¯­ä¹‰æå–)
    * åŒå¡”çš„è¾“å‡ºç”¨
    

åŒå¡”æ¨¡å‹çš„è®­ç»ƒ
â€¢ Pointwise:ç‹¬â½´çœ‹å¾…æ¯ä¸ªæ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬ï¼Œåšç®€å•çš„â¼†å…ƒåˆ†ç±»ã€‚
    â€¢ æŠŠå¬å›çœ‹åšâ¼†å…ƒåˆ†ç±»ä»»åŠ¡ã€‚
    â€¢ å¯¹äºæ­£æ ·æœ¬,â¿åŠ±cos(a,b+) æ¥è¿‘+1ã€‚
    â€¢ å¯¹äºè´Ÿæ ·æœ¬,â¿åŠ±cos(a,b-)æ¥è¿‘-1ã€‚
    â€¢ æ§åˆ¶æ­£è´Ÿæ ·æœ¬æ•°é‡ä¸º1: 2æˆ–è€…1: 3ã€‚

â€¢ Pairwise:æ¯æ¬¡å–â¼€ä¸ªæ­£æ ·æœ¬ã€â¼€ä¸ªè´Ÿæ ·æœ¬[1]ã€‚
    åŸºæœ¬æƒ³æ³•:â¿åŠ±cos(a,b+)â¼¤äºcos(a,b-)
    Triplet hinge loss:
        â€¢ å¦‚æœcos(a, b+)â¼¤äºcos(a,b-)+ ğ‘šï¼Œåˆ™æ²¡æœ‰æŸå¤±ã€‚
        â€¢ å¦åˆ™,æŸå¤±ç­‰äºcos(a,b-) + ğ‘š - cos(a,b+) ã€‚

â€¢ Listwise:æ¯æ¬¡å–â¼€ä¸ªæ­£æ ·æœ¬ã€å¤šä¸ªè´Ÿæ ·æœ¬[2]ã€‚

"""

import torch
import torch.nn as nn

__all__ = ['DSSM', 'DefaultItemTower', 'DefaultUserTower', 
           'TripletHingeLoss', 'TripletLogisticLoss',
           'CrossEntropyLoss']

# æ³¨æ„
# ç‰¹å¾é¢„å¤„ç†ï¼ˆåªåŒºåˆ†ä¸ºä¸¤ç±»ï¼‰
    # dense feaï¼ˆç»„æˆä¸€ç»„å‘é‡ï¼Œç„¶åä½¿ç”¨ä½é˜¶çš„å…¨è¿æ¥å±‚ï¼‰
    # emb fea(ç±»åˆ«idå·)

class DefaultItemTower(nn.Module):
    '''
        ç‰©å“å¡”æ¨¡å‹ç¡®ä¿å¯ä»¥å¤„ç†
            item_emb [B,N,K_item] æ­£æ ·æœ¬N=0ï¼Œå…¶ä»–æ ·æœ¬N=1,2,3...
        ä¿è¯ç”¨æˆ·å¡”å’Œç‰©å“å¡”è¾“å‡ºembç»´åº¦ä¸€è‡´
    '''
    def __init__(self, in_dim, hidden_dims, activation_fun=nn.ReLU()) -> None:
        super(DefaultItemTower, self).__init__()
        
        self.out_emb_dim = hidden_dims[-1]
        
        layers = []
        for h_dim in hidden_dims:
            layers.append(nn.Linear(in_features=in_dim, out_features=h_dim, bias=True))
            layers.append(activation_fun)
            in_dim = h_dim
        self.nns =  nn.Sequential(*layers)

    def forward(self, x):
        # x: item_emb [B,N,K_item]
        B,N,K = x.shape
        h = self.nns(x.reshape(B*N, K)).reshape(B, N, self.out_emb_dim) # h [B, N, out_dim]
        return h

class DefaultUserTower(nn.Module):
    '''
        ç”¨æˆ·å¡”æ¨¡å‹ç¡®ä¿å¯ä»¥å¤„ç†
            user_emb [B, K_user]
        ä¿è¯ç”¨æˆ·å¡”å’Œç‰©å“å¡”è¾“å‡ºembç»´åº¦ä¸€è‡´
    '''
    def __init__(self, in_dim, hidden_dims, activation_fun=nn.ReLU()) -> None:
        super(DefaultUserTower, self).__init__()
        
        layers = []
        for h_dim in hidden_dims:
            layers.append(nn.Linear(in_features=in_dim, out_features=h_dim, bias=True))
            layers.append(activation_fun)
            in_dim = h_dim
        self.nns =  nn.Sequential(*layers)

    def forward(self, x):
        # x: user_emb [B, K_user]
        return self.nns(x)
    

class DSSM(nn.Module):
    '''
        ç‰©å“å¡”æ¨¡å‹ç¡®ä¿å¯ä»¥å¤„ç†
            item_emb [B,N,K_item] æ­£æ ·æœ¬N=0ï¼Œå…¶ä»–æ ·æœ¬N=1,2,3...
        ç”¨æˆ·å¡”æ¨¡å‹ç¡®ä¿å¯ä»¥å¤„ç†
            user_emb [B, K_user]
        assert ç”¨æˆ·å¡”å’Œç‰©å“å¡”è¾“å‡ºembç»´åº¦ä¸€è‡´
        
        ç‰¹åˆ«æ³¨æ„ï¼šè¾“å…¥çš„fake dataæ˜¯embï¼Œæ‰€ä»¥æ¨¡å‹ä¸ä¼šå¯¹ç¼–ç å±‚åšå‚æ•°ä¼˜åŒ–ã€‚
    '''
    def __init__(self, item_tower, user_tower) -> None:
        super(DSSM, self).__init__()
        self.item_tower = item_tower
        self.user_tower = user_tower

    def forward(self, x):
        # user_emb [B, K_user]
        # item_emb [B,N,K_item] æ­£æ ·æœ¬N=0ï¼Œå…¶ä»–æ ·æœ¬N=1,2,3...
        user_emb, item_emb = x
        h_user = self.user_tower(user_emb) # [B, K_out]
        h_item = self.item_tower(item_emb) # [B, N, K_out]
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cos_sim = torch.einsum('bi,bji->bj', h_user, h_item) # ç¡®è®¤ä¸€ä¸‹è¾“å‡º [B, N]
        return cos_sim


# TODO notebook å¯è§†åŒ–åˆ†æ
class TripletHingeLoss(nn.Module):
    def __init__(self, m):
        super(TripletHingeLoss, self).__init__()
        self.m = m

    def forward(self, pos_cosin, neg_cosin):
        # pos_cosin [N], neg_cosin [N]
        assert pos_cosin.shape == neg_cosin.shape
        N = pos_cosin.shape[0]
                
        N_loss = torch.clamp(self.m - (pos_cosin-neg_cosin), min=0)
        loss = torch.sum(N_loss)/N
        return loss

# TODO notebook å¯è§†åŒ–åˆ†æ
class TripletLogisticLoss(nn.Module):
    def __init__(self, sigma):
        super(TripletLogisticLoss, self).__init__()
        self.sigma = sigma

    def forward(self, pos_cosin, neg_cosin):
        # pos_cosin [N], neg_cosin [N]
        assert pos_cosin.shape == neg_cosin.shape
        N = pos_cosin.shape[0]
                
        N_loss = torch.log(1 + torch.exp(self.sigma*(pos_cosin-neg_cosin)))
        loss = torch.sum(N_loss)/N
        return loss

class CrossEntropyLoss(nn.Module):
    def __init__(self):
        super(CrossEntropyLoss, self).__init__()

    def forward(self, x):
        # x:[B, N]
        N = x.shape[0]
        N_loss= -1*torch.log(torch.softmax(x, dim=1)[:,0]) # ç¬¬ä¸€åˆ—ä¸ºæ­£æ ·æœ¬
        loss = torch.sum(N_loss)/N
        return loss