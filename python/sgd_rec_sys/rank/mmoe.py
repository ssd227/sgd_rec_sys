"""
MMOE

å®ç°ç»†èŠ‚ï¼š
    * ä¸è€ƒè™‘embedding layerçš„æ¨¡å‹è®­ç»ƒ
    * FakeMultiTaskDataFactoryç›´æ¥åˆæˆconcatenationåçš„æ•°æ®
    * å…±å››ä¸ªè¾“å‡ºå¤´-TaskHeadï¼ˆç‚¹å‡»ç‡ï¼Œç‚¹èµç‡ï¼Œæ”¶è—ç‡ï¼Œè½¬å‘ç‡ï¼‰
    * æ¯ä¸ªè¾“å‡ºå¤´å¯¹åº”ä¸€ä¸ªTaskGateï¼Œç”¨æ¥mergeä¸“å®¶è¾“å‡ºã€‚
    
    * Expert å¤šç›®æ ‡ä¸“å®¶æ¨¡å‹å¯ä»¥æ›¿æ¢æˆä»»æ„fea-crossæ¨¡å‹ï¼Œ
        ä¸“å®¶æ•°é‡æ˜¯è¶…å‚æ•°ï¼Œè¿™é‡Œé»˜è®¤ä½¿ç”¨MLPã€‚
    
    * TaskGateä¸­ï¼ŒSoftmaxè¾“å‡ºçš„ğ‘› ä¸ªæ•°å€¼è¢«mask çš„æ¦‚ç‡éƒ½æ˜¯10%ã€‚
        æ¯ä¸ªâ€œä¸“å®¶â€è¢«éšæœºä¸¢å¼ƒçš„æ¦‚ç‡éƒ½æ˜¯10%ã€‚

    TODO
        * gate netå¦‚æœç®€åŒ–ä¸ºMLP, å¯ä»¥é€šè¿‡å‚ç›´å åŠ çŸ©é˜µï¼Œç»“æœå‚ç›´åˆ†å‰²åå°±æ˜¯å„ä¸ªgateçš„è¾“å‡ºå€¼ï¼ˆp1,p2,p3...ï¼‰
        * expert net åŒç†ä¹Ÿå¯ä»¥è¿›è¡Œç›¸åŒçš„å¹¶è¡Œè®¡ç®—
            ä½†æ˜¯é€šè¿‡torch.einsumå¥½åƒè®¡ç®—æ•ˆç‡æ›´åŠ ç”ŸçŒ›ï¼ˆå¾—ç ”ç©¶ä¸‹è¿™ä¸ªå‡½æ•°çš„åŸç†å’Œæ­£ç¡®æ€§ï¼‰
        * head net å¯¹åº” N_taskä¸ª inputå•ç‹¬æ“ä½œï¼Œ å¦‚ä½•æé«˜å¹¶è¡Œåº¦å‘¢
        
"""

import torch
import torch.nn as nn

__all__ = ['MMOE', 'Expert', 'TaskHead', 'TaskGate']


def TaskGate(in_dim, hidden_dims, expert_num, activation_fun=nn.ReLU(), dropout_p=0.1):
    layers = []
    for h_dim in hidden_dims: # ä¸åŒ…å«æœ€åä¸€å±‚çš„ç»“æ„
        layers.append(nn.Linear(in_features=in_dim, out_features=h_dim, bias=True))
        layers.append(activation_fun)
        in_dim = h_dim

    # last layer
    layers.append(nn.Linear(in_features=in_dim, out_features=expert_num, bias=True))
    layers.append(nn.Softmax()) # è¾“å‡ºæ¦‚ç‡p [0, 1]
    layers.append(nn.Dropout(p=dropout_p)) # æŒ‰ç…§10%çš„æ¦‚ç‡ä¸¢å¼ƒä¸“å®¶ï¼Œé˜²æ­¢æåŒ–é—®é¢˜
    
    return nn.Sequential(*layers)

def TaskHead(in_dim, hidden_dims, activation_fun=nn.ReLU()):
    layers = []
    for h_dim in hidden_dims: # ä¸åŒ…å«æœ€åä¸€å±‚çš„ç»“æ„
        layers.append(nn.Linear(in_features=in_dim, out_features=h_dim, bias=True))
        layers.append(activation_fun)
        in_dim = h_dim

    # last layer
    layers.append(nn.Linear(in_features=in_dim, out_features=1, bias=True))
    layers.append(nn.Softmax()) # è¾“å‡ºæ¦‚ç‡p [0, 1]
    
    return nn.Sequential(*layers)

def Expert(in_dim, hidden_dims, activation_fun=nn.ReLU()):   
    layers = []
    for h_dim in hidden_dims:
        layers.append(nn.Linear(in_features=in_dim, out_features=h_dim, bias=True))
        layers.append(activation_fun)
        in_dim = h_dim
    return nn.Sequential(*layers)


class MMOE(nn.Module):
    def __init__(self, expert_nets, heads_nets, gate_nets) -> None:
        super(MMOE, self).__init__()
        self.experts = expert_nets # ä¸“å®¶ç½‘ç»œå¯ä»¥è‡ªå®šä¹‰
        self.heads = heads_nets
        self.gates = gate_nets

    def forward(self, x):
        # TODO ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œè¿­ä»£ä¸å¤ªè¡Œ
        h_expert_list = torch.stack([expert(x) for expert in self.experts], dim=1) # [B, N_expert, K]
        h_gate_list = torch.stack([gate(x) for gate in self.gates], dim=1) # [B, Task_num, N_expert]

        head_input = torch.einsum('bek, bte -> btk', h_expert_list, h_gate_list) # [B, Task_num, K]

        result = [self.heads[i](head_input[:,i,:]) for i in range(len(self.heads))]
        return torch.cat(result, dim=1) # out [B,N_heads]
            

class CrossEntropyLoss(nn.Module):
    def __init__(self):
        super(CrossEntropyLoss, self).__init__()
        '''
        # ä¸ºäº†é¿å…æ— ç©·å¤§ï¼Œæ•°æ®æº¢å‡ºçš„é—®é¢˜ï¼Œnn.BCELossåšäº†æˆªæ–­å¤„ç†
            "Our solution is that BCELoss clamps its log function outputs 
            to be greater than or equal to -100. This way, we can always 
            have a finite loss value and a linear backward method."

            from https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html
        '''

        self.bce_loss = nn.BCELoss(reduction='none')

    def forward(self, ps, ys):
        # è¾“å…¥ç»´åº¦ [B, head_num]
        assert ps.shape == ys.shape
        N = ps.shape[0]
        
        loss_mat = self.bce_loss(ps,ys) # å…ƒç´ çº§äºŒå…ƒäº¤å‰ç†µ [B, head_num]
        return torch.sum(loss_mat)/N    # ç¬¦åˆç‹æ ‘æ£®è®²ä¹‰ä¸Šçš„å®šä¹‰(æ¯ä¸ªæ ·æœ¬ä¸Šï¼Œmulti task headä¸Šçš„äºŒå…ƒäº¤å‰ç†µlosså–sumï¼Œbatchå†…losså–mean)

        # æŒ‰ç…§ä¸‹é¢å…¬å¼å®ç°ï¼Œæ•°å€¼ä¸Šå¯èƒ½ä¼šçˆ†æ‰  
        # N_loss = -(ys * torch.log2(ps) + (1-ys) * torch.log2(1-ps))  
