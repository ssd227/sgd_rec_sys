{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵分解or 矩阵补充算法（过时&不适合做召回）\n",
    "\n",
    "原理\n",
    "1) 计算物品emb 和用户emb\n",
    "2) 使用dot计算 user_emb, item_emb见的内积\n",
    "3) 内积结果用来拟合user给item的打分值\n",
    "\n",
    "缺点：\n",
    "1) 仅⽤ID embedding，没利⽤物品、⽤户属性\n",
    "2) 负样本：曝光之后，没有点击、交互。（错误的做法）\n",
    "3) 做训练的⽅法不好(回归不如分类，内积不如余弦相似度)\n",
    "4) 对新物品不友好( 新物品没有展现数据，rate数据，更没有训练好的emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/playground/sgd_deep_learning/sgd_rec_sys\n"
     ]
    }
   ],
   "source": [
    "%cd /playground/sgd_deep_learning/sgd_rec_sys/\n",
    "import sys \n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sgd_rec_sys.retrieval import MF, RateInfo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 2, 2, 3, 3, 3]) tensor([1, 2, 0, 2, 1, 3, 1, 2, 3]) tensor([4.5000, 2.0000, 4.0000, 3.5000, 5.0000, 2.0000, 3.5000, 4.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# TODO: 改写成DataSet 和 DataLoader\n",
    "def load_data():\n",
    "    data = [(1, 2, 4.5),\n",
    "            (1, 3, 2.0),\n",
    "            (2, 1, 4.0), \n",
    "            (2, 3, 3.5),\n",
    "            (3, 2, 5.0),\n",
    "            (3, 4, 2.0),\n",
    "            (4, 2, 3.5), \n",
    "            (4, 3, 4.0), \n",
    "            (4, 4, 1.0)]\n",
    "    uids = [x[0] - 1 for x in data]\n",
    "    iids = [x[1] - 1 for x in data]\n",
    "    scores = [x[2] for x in data]\n",
    "\n",
    "    return torch.LongTensor(uids), torch.LongTensor(iids),  torch.Tensor(scores)\n",
    "\n",
    "user_ids, item_ids, scores = load_data() \n",
    "print(user_ids, item_ids, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, loss_fn, optimizer):\n",
    "    user_ids, item_ids, scores = train_x\n",
    "    y = model.forward(user_ids, item_ids)\n",
    "    out = loss_fn(y, scores)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    out.backward()\n",
    "    optimizer.step()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MF(\n",
      "  (user_emb): Embedding(4, 32)\n",
      "  (item_emb): Embedding(4, 32)\n",
      ")\n",
      "0 loss: tensor(18.5207, grad_fn=<MseLossBackward0>)\n",
      "1 loss: tensor(15.8223, grad_fn=<MseLossBackward0>)\n",
      "2 loss: tensor(13.3671, grad_fn=<MseLossBackward0>)\n",
      "3 loss: tensor(11.1465, grad_fn=<MseLossBackward0>)\n",
      "4 loss: tensor(9.1638, grad_fn=<MseLossBackward0>)\n",
      "5 loss: tensor(7.4099, grad_fn=<MseLossBackward0>)\n",
      "6 loss: tensor(5.8869, grad_fn=<MseLossBackward0>)\n",
      "7 loss: tensor(4.5793, grad_fn=<MseLossBackward0>)\n",
      "8 loss: tensor(3.4899, grad_fn=<MseLossBackward0>)\n",
      "9 loss: tensor(2.6018, grad_fn=<MseLossBackward0>)\n",
      "10 loss: tensor(1.8963, grad_fn=<MseLossBackward0>)\n",
      "11 loss: tensor(1.3427, grad_fn=<MseLossBackward0>)\n",
      "12 loss: tensor(0.9196, grad_fn=<MseLossBackward0>)\n",
      "13 loss: tensor(0.5939, grad_fn=<MseLossBackward0>)\n",
      "14 loss: tensor(0.3528, grad_fn=<MseLossBackward0>)\n",
      "15 loss: tensor(0.1799, grad_fn=<MseLossBackward0>)\n",
      "16 loss: tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "17 loss: tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
      "18 loss: tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "19 loss: tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "20 loss: tensor(0.0031, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MF(userN=4, itemN=4, dim=32)\n",
    "print(mf)\n",
    "\n",
    "training_data = load_data()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(mf.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "best_states = None\n",
    "best_loss = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer = torch.optim.Adam(mf.parameters(), lr=1e-2, weight_decay=0.1)\n",
    "    loss = train(model=mf, train_x=training_data, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    if best_loss is not None and loss > best_loss:\n",
    "        continue\n",
    "    else:\n",
    "        best_loss = loss\n",
    "        best_states = mf.state_dict()\n",
    "    \n",
    "    print(epoch, 'loss:', loss)\n",
    "\n",
    "# load best parameters\n",
    "mf.load_state_dict(best_states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证训练集效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user[0], item[2], pred_rate[2.0], True_rate[tensor([2.0260], grad_fn=<SumBackward1>)]\n"
     ]
    }
   ],
   "source": [
    "# 重复运行，查看不同样本预测情况\n",
    "k = random.randint(0, len(training_data[0])-1)\n",
    "user_id, item_id, rate_score = [training_data[i][k] for i in range(3)]\n",
    "\n",
    "pred_y = mf.forward(user_id.unsqueeze(-1), item_id.unsqueeze(-1))\n",
    "print(\"user[{}], item[{}], pred_rate[{}], True_rate[{}]\".format(user_id, item_id, rate_score, pred_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emb线上使用流程\n",
    "* user emb 使用userid 即可查找\n",
    "* item emb 需要使用Milvus、Faiss、HnswLib等向量数据库，方便快速找到与query(user emb)内积最大的topk物品"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
