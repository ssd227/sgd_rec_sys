{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双塔模型\n",
    "1) Pointwise 训练\n",
    "2) Pairwise 训练\n",
    "3) Listwise 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/playground/sgd_deep_learning/sgd_rec_sys\n"
     ]
    }
   ],
   "source": [
    "%cd /playground/sgd_deep_learning/sgd_rec_sys/\n",
    "import sys \n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sgd_rec_sys.data import FakeDssmDataFactory, DssmDataset\n",
    "from sgd_rec_sys.retrieval import DSSM, DefaultItemTower, DefaultUserTower, \\\n",
    "TripletHingeLoss, TripletLogisticLoss, CrossEntropyLoss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Pointwise训练\n",
    "把召回看做⼆元分类任务。\n",
    "* 对于正样本，⿎励cos 𝐚, 𝐛 接近+1。\n",
    "* 对于负样本，⿎励cos 𝐚, 𝐛 接近−1。\n",
    "* 控制正负样本数量为1: 2或者1: 3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现有框架下，fake数据需要重新构造， 比较简单，暂时pass\n",
    "# 使用交叉熵预测正负样本(1，0)。 \n",
    "\n",
    "## 数据准备\n",
    "## 参数设置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pairwise训练\n",
    "\n",
    "```\n",
    "    针对pairwise自定义了TripletHingeLoss\n",
    "        input: \n",
    "            cos(a,b+), cos(a,b-)\n",
    "            两个cos值已在dssm里计算好了\n",
    "\n",
    "        超参数：\n",
    "            triplet_hinge_loss_m，需要进行网格搜索\n",
    "            \n",
    "            默认设置为1，由于cos的取值在[-1,1], \n",
    "            极限条件下正样本为1，负样本为-1，间隔为2，这里取中间间隔1\n",
    "\n",
    "    from chatgpt:\n",
    "        在 Triplet Hinge Loss 中，参数 m 是一个超参数，用于控制正例和负例之间的间隔。\n",
    "        通常情况下，m 的选择会影响模型的性能和训练稳定性。\n",
    "        \n",
    "        选择合适的 m 值通常需要根据具体的任务和数据集进行调整和优化。\n",
    "        一般来说，较小的 m 值会使得模型更加关注于难以区分的样本，从而更好地推动模型向着更好的方向训练。\n",
    "        但是，如果选择过小的 m 值，可能会导致模型过拟合或训练不稳定。\n",
    "\n",
    "        相反，较大的 m 值会使得模型更加关注于易于区分的样本，\n",
    "        从而可能导致模型过度简化或者忽视难以区分的样本。\n",
    "        但是，选择过大的 m 值可能会导致模型难以收敛或者陷入局部最优。\n",
    "\n",
    "        因此，选择合适的 m 值需要在模型的训练过程中进行实验和调整。\n",
    "        您可以尝试使用交叉验证或者网格搜索等方法来选择最佳的 m 值，以优化模型的性能。\n",
    "        通常情况下，m 的取值范围可以从一个小的正数开始尝试，然后根据实际效果逐步调整。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user feature embedding success, shape: (1000, 80)\n",
      "item feature embedding success, shape: (1000, 2, 90)\n"
     ]
    }
   ],
   "source": [
    "## 数据准备 ##\n",
    "\n",
    "# input\n",
    "n_samples = 1000 # 总样本数\n",
    "user_fea_dim = 80\n",
    "item_fea_dim = 90\n",
    "item_fea_num = 2 # （正样本1: 负样本1）\n",
    "\n",
    "## 生成伪CTR数据\n",
    "dump_file = './data/fake/tmp_dssm.pkl'\n",
    "fake_data_factory = FakeDssmDataFactory(n_samples,\n",
    "                                        user_fea_dim,\n",
    "                                        item_fea_dim,\n",
    "                                        item_fea_num,\n",
    "                                        dtype=np.float32)\n",
    "fake_data_factory.presist(dump_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "DSSM(\n",
      "  (item_tower): DefaultItemTower(\n",
      "    (nns): Sequential(\n",
      "      (0): Linear(in_features=90, out_features=180, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=180, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (user_tower): DefaultUserTower(\n",
      "    (nns): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=160, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=160, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 参数设置 ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "\n",
    "# 训练参数\n",
    "train_batch_size = 64\n",
    "epochs = 5\n",
    "triplet_hinge_loss_m = 1 # 超参数需要网格搜索\n",
    "\n",
    "with open(dump_file,'rb') as f:\n",
    "    fake_data = pickle.load(f)\n",
    "train_ds = DssmDataset(data_info=fake_data, device=device)\n",
    "train_dl = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# 网络结构\n",
    "out_dim = 128 # 在128维度上做内积或者cosin相似度\n",
    "user_hidden_dims = [int(user_fea_dim*2), 1024, 256, out_dim]\n",
    "item_hidden_dims = [int(item_fea_dim*2), 1024, 256, out_dim]\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "user_tower = DefaultUserTower(in_dim=user_fea_dim,\n",
    "                              hidden_dims = user_hidden_dims,\n",
    "                              activation_fun=nn.ReLU())\n",
    "                             \n",
    "item_tower = DefaultItemTower(in_dim=item_fea_dim, \n",
    "                              hidden_dims= item_hidden_dims, \n",
    "                              activation_fun=nn.ReLU())\n",
    "\n",
    "model = DSSM(item_tower=item_tower,\n",
    "             user_tower= user_tower,).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = TripletHingeLoss(m=triplet_hinge_loss_m)  # Pairwise loss\n",
    "# criterion = TripletLogisticLoss(sigma=1)  # Pairwise loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD 优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0012\n",
      "Epoch [2/5], Loss: 0.9996\n",
      "Epoch [3/5], Loss: 0.9999\n",
      "Epoch [4/5], Loss: 1.0002\n",
      "Epoch [5/5], Loss: 1.0001\n"
     ]
    }
   ],
   "source": [
    "## 训练 ##\n",
    "def train(dataloader, model, epochs=1,):\n",
    "    for epoch in range(epochs):\n",
    "        for x in dataloader:\n",
    "            # 前向传播\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs[:,0], outputs[:,1])\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "train(train_dl, model, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Listwise训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user feature embedding success, shape: (1000, 80)\n",
      "item feature embedding success, shape: (1000, 10, 90)\n"
     ]
    }
   ],
   "source": [
    "## 数据准备 ##\n",
    "\n",
    "# input\n",
    "n_samples = 1000 # 总样本数\n",
    "user_fea_dim = 80\n",
    "item_fea_dim = 90\n",
    "item_fea_num = 10 # （正样本1: 负样本9）\n",
    "\n",
    "## 生成伪CTR数据\n",
    "dump_file = './data/fake/tmp_dssm.pkl'\n",
    "fake_data_factory = FakeDssmDataFactory(n_samples,\n",
    "                                        user_fea_dim,\n",
    "                                        item_fea_dim,\n",
    "                                        item_fea_num,\n",
    "                                        dtype=np.float32)\n",
    "fake_data_factory.presist(dump_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "DSSM(\n",
      "  (item_tower): DefaultItemTower(\n",
      "    (nns): Sequential(\n",
      "      (0): Linear(in_features=90, out_features=180, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=180, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (user_tower): DefaultUserTower(\n",
      "    (nns): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=160, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=160, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 参数设置 ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "\n",
    "# 训练参数\n",
    "train_batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "with open(dump_file,'rb') as f:\n",
    "    fake_data = pickle.load(f)\n",
    "train_ds = DssmDataset(data_info=fake_data, device=device)\n",
    "train_dl = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# 网络结构\n",
    "out_dim = 128 # 在128维度上做内积或者cosin相似度\n",
    "user_hidden_dims = [int(user_fea_dim*2), 1024, 256, out_dim]\n",
    "item_hidden_dims = [int(item_fea_dim*2), 1024, 256, out_dim]\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "user_tower = DefaultUserTower(in_dim=user_fea_dim,\n",
    "                              hidden_dims = user_hidden_dims,\n",
    "                              activation_fun=nn.ReLU())\n",
    "                             \n",
    "item_tower = DefaultItemTower(in_dim=item_fea_dim, \n",
    "                              hidden_dims= item_hidden_dims, \n",
    "                              activation_fun=nn.ReLU())\n",
    "\n",
    "model = DSSM(item_tower=item_tower,\n",
    "             user_tower= user_tower,).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD 优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3040\n",
      "Epoch [2/5], Loss: 2.3029\n",
      "Epoch [3/5], Loss: 2.3036\n",
      "Epoch [4/5], Loss: 2.3039\n",
      "Epoch [5/5], Loss: 2.3008\n"
     ]
    }
   ],
   "source": [
    "## 训练 ##\n",
    "def train(dataloader, model, epochs=1,):\n",
    "    for epoch in range(epochs):\n",
    "        for x in dataloader:\n",
    "            # 前向传播\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "train(train_dl, model, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
