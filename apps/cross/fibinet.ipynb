{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiBiNet\n",
    "\n",
    "下述流程，暂不考虑连续特征\n",
    "- 假设所有连续特征都做了离散化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/playground/sgd_deep_learning/sgd_rec_sys\n"
     ]
    }
   ],
   "source": [
    "%cd /playground/sgd_deep_learning/sgd_rec_sys/\n",
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sgd_rec_sys.cross import FiBiNet\n",
    "from sgd_rec_sys.data import FakeCtrDataFactory, CtrDataset011, ctr_collate_fn_011"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTR fake data生成\n",
    "```\n",
    " multi-hot的最大采样数可以在FakeDataFactory中设置，默认为4\n",
    "    def make_multi_hot_fea(self, n_samples, fea_list, max_len=4):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot feas success, shape: (1000, 4)\n",
      "multi-hot feas success\n",
      "label success, shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "n_samples = 1000 # 总样本数\n",
    "n_dense_fea = 0 # 0 不生成连续数据\n",
    "embedding_dim = K = 32 # fibinet所有特征维度固定，统一为设为K\n",
    "\n",
    "one_hot_fea_list = [100, 34, 42, 10] # 4个fea, 字典大小分别为100，34，42，10\n",
    "multi_hot_fea_list = [10, 20, 30] # 3个fea，字典大小分别为10，20，30\n",
    "\n",
    "fields_num = F = len(one_hot_fea_list) + len(multi_hot_fea_list) # 总特征数\n",
    "fields_dim = int(F*(1+F)/2 * K) #离散特征cross后,总编码维度\n",
    "\n",
    "## 生成伪CTR数据\n",
    "dump_file = './data/fake/tmp.pkl'\n",
    "fake_data_factory = FakeCtrDataFactory(n_samples,\n",
    "                                    n_dense_fea,\n",
    "                                    one_hot_fea_list,\n",
    "                                    multi_hot_fea_list,\n",
    "                                    dtype=np.float32)\n",
    "fake_data_factory.presist(dump_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "\n",
    "# 训练参数\n",
    "train_batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "with open(dump_file,'rb') as f:\n",
    "    fake_data = pickle.load(f)\n",
    "train_ds = CtrDataset011(fake_data)\n",
    "train_dl = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, collate_fn=ctr_collate_fn_011(device))\n",
    "\n",
    "# 网络结构\n",
    "hidden_dims = [fields_dim*4, 1024, 256] # deep net 不包括分类层\n",
    "reduction_ratio = 2 # senet中，类似autoencoder中间表示维度 = input_dim/reduction_ratio\n",
    "\n",
    "# 定义模型\n",
    "model = FiBiNet(senet_r=2,\n",
    "                fix_emb_dim=K, # fibinet所有特征维度固定，统一为设为K\n",
    "                deepnet_hidden_dims=hidden_dims,\n",
    "                one_hot_fea_list=one_hot_fea_list,\n",
    "                multi_hot_fea_list=multi_hot_fea_list,\n",
    "                ).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()  # 二分类交叉熵损失函数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD 优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=0.3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6925\n",
      "Epoch [2/5], Loss: 0.6926\n",
      "Epoch [3/5], Loss: 0.6937\n",
      "Epoch [4/5], Loss: 0.6938\n",
      "Epoch [5/5], Loss: 0.6921\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, epochs=1,):\n",
    "    for epoch in range(epochs):\n",
    "        for x in dataloader:\n",
    "            y, one_hot_x, multi_hot_x = x\n",
    "            # 前向传播\n",
    "            outputs = model((one_hot_x, multi_hot_x))\n",
    "            loss = criterion(outputs, y.reshape(-1,1))\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "train(train_dl, model, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fibinet使用演示 END\n",
    "\n",
    "---\n",
    "## Tricks in bilinear interaction layer\n",
    "    下述tricks与fibinet的实现细节相关，可以跳过\n",
    "\n",
    "* tirck1: for self.w_type == 'field-all':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2, 3, 1]) torch.Size([2, 1, 3])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[  1,   2,   3],\n",
      "         [  2,   4,   6],\n",
      "         [  3,   6,   9]],\n",
      "\n",
      "        [[100, 200, 300],\n",
      "         [200, 400, 600],\n",
      "         [300, 600, 900]]])\n",
      "torch.Size([3, 3, 2])\n",
      "tensor([[[  1, 100],\n",
      "         [  2, 200],\n",
      "         [  3, 300]],\n",
      "\n",
      "        [[  2, 200],\n",
      "         [  4, 400],\n",
      "         [  6, 600]],\n",
      "\n",
      "        [[  3, 300],\n",
      "         [  6, 600],\n",
      "         [  9, 900]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "'''\n",
    "演示\n",
    "    A1 [K,F,1] @ A2 [K,1,F] \n",
    "        => out1 [K,F,F]\n",
    "        => out2 [F,F,K]\n",
    "'''\n",
    "F, K = 3,2\n",
    "A = torch.tensor([[1,10],\n",
    "                  [2,20],\n",
    "                  [3,30]])\n",
    "print(A.shape)\n",
    "\n",
    "# 将矩阵 A 和 B 分别增加一个维度，然后执行元素乘积\n",
    "A1 = A.unsqueeze(1).permute((2,0,1))  # 在第二个维度上增加一个维度\n",
    "A2 = A.unsqueeze(0).permute((2,0,1))  # 在第一个维度上增加一个维度\n",
    "print(A1.shape, A2.shape)\n",
    "\n",
    "out1 = A1 @ A2\n",
    "print(out1.shape)\n",
    "print(out1)\n",
    "\n",
    "out2 = out1.permute((1,2, 0))\n",
    "print(out2.shape)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "torch.Size([2, 2, 3, 1]) torch.Size([2, 2, 1, 3])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "tensor([[[[   1,    2,    3],\n",
      "          [   2,    4,    6],\n",
      "          [   3,    6,    9]],\n",
      "\n",
      "         [[ 100,  200,  300],\n",
      "          [ 200,  400,  600],\n",
      "          [ 300,  600,  900]]],\n",
      "\n",
      "\n",
      "        [[[  16,   20,   24],\n",
      "          [  20,   25,   30],\n",
      "          [  24,   30,   36]],\n",
      "\n",
      "         [[1681, 2091, 2501],\n",
      "          [2091, 2601, 3111],\n",
      "          [2501, 3111, 3721]]]])\n",
      "torch.Size([2, 3, 3, 2])\n",
      "tensor([[[  1, 100],\n",
      "         [  2, 200],\n",
      "         [  3, 300]],\n",
      "\n",
      "        [[  2, 200],\n",
      "         [  4, 400],\n",
      "         [  6, 600]],\n",
      "\n",
      "        [[  3, 300],\n",
      "         [  6, 600],\n",
      "         [  9, 900]]])\n",
      "tensor([[[  16, 1681],\n",
      "         [  20, 2091],\n",
      "         [  24, 2501]],\n",
      "\n",
      "        [[  20, 2091],\n",
      "         [  25, 2601],\n",
      "         [  30, 3111]],\n",
      "\n",
      "        [[  24, 2501],\n",
      "         [  30, 3111],\n",
      "         [  36, 3721]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "'''\n",
    "演示\n",
    "    Vi [B,K,F,1] @ W [B,K,1,F] \n",
    "        => out1 [B,K,F,F]\n",
    "        => out2 [B,F,F,K]\n",
    "'''\n",
    "\n",
    "# 示例矩阵 A 和 B\n",
    "B,F,K =2,3,2\n",
    "A = torch.tensor([[[1,10],[2,20],[3,30]],\n",
    "                  [[4,41],[5, 51],[6,61]]])\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "# 将矩阵 A 和 B 分别增加一个维度，然后执行元素乘积\n",
    "A1 = A.unsqueeze(2).permute((0,3,1,2))  # 在第二个维度上增加一个维度\n",
    "A2 = A.unsqueeze(1).permute((0,3,1,2))  # 在第一个维度上增加一个维度\n",
    "print(A1.shape, A2.shape)\n",
    "\n",
    "out1 = A1@A2\n",
    "print(out1.shape)\n",
    "print(out1)\n",
    "\n",
    "out2 = out1.permute((0,2,3,1))\n",
    "print(out2.shape)\n",
    "print(out2[0])\n",
    "print(out2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False],\n",
      "        [ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "Lower triangle without zeros as 1D tensor:\n",
      "tensor([1, 4, 5, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "'''\n",
    "演示\n",
    "    利用mask从[F,F]中提取下三角矩阵元素\n",
    "'''\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "# 生成下三角部分的逻辑掩码\n",
    "mask = torch.tril(torch.ones(3,3, dtype=torch.bool))\n",
    "print(mask)\n",
    "\n",
    "# 使用逻辑掩码提取下三角部分并展平为一维张量\n",
    "lower_triangle_1d = matrix[mask]\n",
    "\n",
    "print(\"Lower triangle without zeros as 1D tensor:\")\n",
    "print(lower_triangle_1d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trick2：self.w_type == 'field-each':\n",
    "\n",
    "e:[B, F, K] => e[B,F,1,K]\n",
    "e[B,F,1,K] @ W [F,K,K] => [B, F, 1, K]  \n",
    "@支持上述操作\n",
    "每个F相互对应, B维度可以自动broadcast\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "tensor([[[[1.0000e+00, 1.0000e+01, 1.0000e+02, 1.0000e+03]],\n",
      "\n",
      "         [[2.0000e+00, 2.0000e+01, 2.0000e+02, 2.0000e+03]],\n",
      "\n",
      "         [[3.0000e+00, 3.0000e+01, 3.0000e+02, 3.0000e+03]]],\n",
      "\n",
      "\n",
      "        [[[5.0000e+00, 5.0000e+01, 5.0000e+02, 5.0000e+03]],\n",
      "\n",
      "         [[1.0000e+01, 1.0000e+02, 1.0000e+03, 1.0000e+04]],\n",
      "\n",
      "         [[1.5000e+01, 1.5000e+02, 1.5000e+03, 1.5000e+04]]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.]],\n",
      "\n",
      "        [[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]]])\n",
      "torch.Size([2, 3, 1, 4])\n",
      "tensor([[[[ 1111.,  1111.,  1111.,  1111.]],\n",
      "\n",
      "         [[ 4444.,  4444.,  4444.,  4444.]],\n",
      "\n",
      "         [[ 9999.,  9999.,  9999.,  9999.]]],\n",
      "\n",
      "\n",
      "        [[[ 5555.,  5555.,  5555.,  5555.]],\n",
      "\n",
      "         [[22220., 22220., 22220., 22220.]],\n",
      "\n",
      "         [[49995., 49995., 49995., 49995.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "'''\n",
    "演示\n",
    "    Vi:[B, F, 1, K] @ Wi: [F, K, K]\n",
    "        => [B, F, 1, K]\n",
    "'''\n",
    "B, F, K = 2, 3, 4\n",
    "\n",
    "A = torch.tensor([[i*(10**j) for j in range(K)] for i in range(1, F+1)], dtype=torch.float32) # FK\n",
    "AA  = torch.stack([A, 5*A],dim=0).reshape(B,F,1,K)\n",
    "print(AA.shape)\n",
    "\n",
    "B = torch.ones(K,K)\n",
    "BB = torch.stack([B,2*B, 3*B], dim=0)\n",
    "print(BB.shape)\n",
    "\n",
    "print(AA)\n",
    "print(BB)\n",
    "\n",
    "C = AA@BB\n",
    "print(C.shape) # [B,F,1, K]\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "演示\n",
    "    自动broadcast\n",
    "    [B, Fi, Fj, K] * [B, 1, Fj, K] => [B, Fi, Fj, K]\n",
    "'''\n",
    "\n",
    "B, Fi, Fj, K = 2,3,4,5\n",
    "\n",
    "a = torch.randn(B,Fi,Fj,K)\n",
    "b = torch.ones(B,1,Fj,K)\n",
    "\n",
    "c= a*b \n",
    "print(c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
